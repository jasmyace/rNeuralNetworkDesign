% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/perceptron.R
\name{perceptron}
\alias{perceptron}
\title{Display perceptron results for two-dimensional targets.}
\usage{
perceptron(p, t, verbose = FALSE, W_0 = NULL, b_0 = NULL)
}
\arguments{
\item{p}{A list of input vectors of uniform size.}

\item{t}{A list of target vectors of uniform size.}

\item{verbose}{A logical indicating if the function should report progress
after each iteration.  Default is \code{FALSE}.}

\item{W_0}{A numeric matrix with initial values.  Default is \code{NULL}, in
which case the zero matrix initializes the algorithm.}

\item{b_0}{A numeric matrix with initial values.  Default is \code{NULL}, in
which case the zero vector initializes the algorithm.}
}
\description{
Display perceptron results in two-dimensional space via
two-dimensional targets.  Decision boundaries, valid regions, and training
data are included.
}
\details{
Functon \code{perceptron} tries to identify a set of linear decision
boundaries that correctly classify a set of inputs.  In the case no such
set of linear decision boundaries exists, nothing will happen without
intervention from the user; i.e., the function will continue to iterate
indefintely.

The function works to identify an optimum weight matrix \eqn{\mathbf{W}} and
bias vector \eqn{\mathbf{b}} such that the \eqn{i^{th}} neuron correctly
classifies that dimension of an input vector with regards to its supervised
target.  The \eqn{i^{th}} row of \eqn{\mathbf{W}} and \eqn{\mathbf{b}}
corresponds to \eqn{\mathbf{w}_i^T} and bias vector \eqn{b_i}, respectively,
meaning that \eqn{n_i = \mathbf{w}_i^T\mathbf{p} + b_i \geq 0} are classified
as \eqn{+1}, while \eqn{\mathbf{w}_i^T\mathbf{p} + b_i < 0} classify as
\eqn{0}.  Thus, the \code{perceptron} function uses a \code{hardlim}
activation function \eqn{f(n_i) = a_i}, where \eqn{a_i} equals \eqn{0} or
\eqn{1}.

Identification of a valid \eqn{\mathbf{W}} and \eqn{\mathbf{b}} proceeds
algorithmically via stochastic descent, meaning that input vectors \code{p}
evaluate one at a time, in order.  If necessary, vectors evaluate multiple
times, in order, until all points correctly match their targets.

Use of the \code{verbse = TRUE} option may be useful to help identify
progress in slow-to-converge or seemingly unsolvable (read: possibly not
linearly separable) problems.  Note that weight matrices report as a long
vector, so care must be taken to ensure appropriate interpretation.
}
\examples{
\dontrun{
p <- list(matrix(c(2, 2), ncol=1), matrix(c(1, -2), ncol=1), 
matrix(c(-2, 2), ncol=1), matrix(c(-1, 1), ncol=1))
t <- list(0, 1, 0, 1)
verbose <- TRUE
perceptron(p, t, verbose)

p <- list(matrix(c(0, 2), ncol=1), matrix(c(1, 0), ncol=1), 
matrix(c(0, -2), ncol=1), matrix(c(2, 0), ncol=1))
t <- list(1, 1, 0, 0)
verbose <- TRUE
perceptron(p, t, verbose)

p <- list(c(1, 1), c(1, 2), c(2, -1), c(2, 0), c(-1, 2), c(-2, 1), c(-1, -1), c(-2 ,-2))
t <- list(c(0, 0), c(0, 0), c(0, 1), c(0, 1), c(1, 0), c(1, 0), c(1, 1), c(1, 1))
verbose <- TRUE
W_0 <- matrix(c(1, 0, 0, 1), ncol = 2)
b_0 <- c(1, 1)
ans <- perceptron(p, t, verbose, W_0, b_0)}

}
\references{
Martin T. Hagan, Howard B. Demuth, Mark H. Beale and Orlando De
JesÃºs. 2014. Neural Network Design (2nd. ed.). Martin Hagan, Stillwater,
OK, USA.
}
\seealso{
\code{perceptron_plot}
}
\author{
Jason Mitchell
}
